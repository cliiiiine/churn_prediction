{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report: Interconnect Churn Prediction Project\n",
    "\n",
    "## 1. Steps performed and skipped\n",
    "\n",
    "### Performed steps: \n",
    "\n",
    "* **Data Loading & Initial Exploration:**\n",
    "    * Loaded the dataset and conducted an initial review to understand the structure, data types, and presence of null values.\n",
    "  \n",
    "* **Data Cleaning:**\n",
    "    * Handled missing values using domain-appropriate strategies (e.g., filling NAs in service-related columns with \"No\").\n",
    "      \n",
    "    * Converted categorical variables such as Yes/No to binary format for model compatibility.\n",
    " \n",
    "      \n",
    "* **EDA (Exploratory Data Analysis):**\n",
    "\n",
    "    * Visualized churn distribution, analyzed feature correlations, and understood the behavior of categorical and numerical features.\n",
    " \n",
    "* **Feature Engineering:**\n",
    "\n",
    "    * One-hot encoded nominal categorical features.\n",
    "\n",
    "    * Created a binary churn column as the target variable.\n",
    " \n",
    "* **Handling Class Imbalance with SMOTE:**\n",
    "    * Applied SMOTE after train-test split to avoid data leakage.\n",
    "    * Balanced the minority (churn) class to improve model generalization.\n",
    " \n",
    "  \n",
    "* **Train/Test Split:**\n",
    "\n",
    "    * Split the dataset into training and testing subsets with stratification on the target variable.\n",
    "    \n",
    "  \n",
    "* **Model Training:**\n",
    "\n",
    "    * Trained 6 models:\n",
    "        * Logistic Regression\n",
    "        * Random Forest\n",
    "        * XGBoost\n",
    "        * Decision Tree\n",
    "        * CatBoost\n",
    "        * LightGBM\n",
    "        \n",
    "\n",
    "* **Model Evaluation:**\n",
    "\n",
    "    * Used metrics like ROC AUC, precision, recall, and F1-score. Also visualized confusion matrices and ROC curves. Ignored accuracy due to imbalance.\n",
    "  \n",
    "* **Hyperparameter Tuning:**\n",
    "\n",
    "    * Used RandomSearchCV for optimizing model performance.\n",
    " \n",
    "### Skipped Steps:\n",
    "\n",
    "* **Deep Learning Models:**\n",
    "    * Skipped due to having a relatively small dataset size. Tree-based or linear models were more suitable.\n",
    "  \n",
    "* **Complex Feature Engineering:**\n",
    "\n",
    "    * Advanced NLP wasn't necessary as most variables were structured and categorical.\n",
    "  \n",
    "* **Production Deployment:**\n",
    "\n",
    "    * Not required for this project scope; focus was on model development and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Difficulties Encountered & Solutions\n",
    "\n",
    "* **Imbalanced Target Classes:**\n",
    "\n",
    "    * Initially, churners were a small portion of the dataset, which hurt recall.\n",
    "    * Solution: Applied SMOTE on the training set to synthetically generate churn samples, which boosted model recall and balanced learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Steps to Solving the Task\n",
    "\n",
    "* **EDA:**\n",
    "    * Understanding churn drivers helped prioritize which features might be most predictive (e.g., contract type, monthly charges).\n",
    "  \n",
    "* **Feature Selection:**\n",
    "\n",
    "    * Dropping low-variance and redundant features improved model performance.\n",
    "  \n",
    "* **Hyperparameter Tuning:**\n",
    "\n",
    "    * Helped fine-tune Random Forest and XGBoost models for higher ROC AUC and better generalization. Although some models attained worse scores after tuning.\n",
    "\n",
    "* **Model Comparison:**\n",
    "\n",
    "    * Testing multiple models revealed the best tradeoff between interpretability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Model & Performance\n",
    "\n",
    "* **Selected Model:** LightGBM Classifier\n",
    "\n",
    "* **Parameters Used:** Tuned via RandomSearchCV on validation set.\n",
    "\n",
    "* **Key Features:** Contract Type, Begin Date (Tenure), Monthly Charges, Payment Method\n",
    "\n",
    "* **Threshold Used:** 0.1 used to increase recall\n",
    "\n",
    "\n",
    "* **Test Set Performance:**\n",
    "\n",
    "    * Precision: 0.41\n",
    "    * Recall: 0.94\n",
    "    * F1-Score: 0.57\n",
    "    * ROC AUC: 0.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The final LightGBM model demonstrates strong recall (0.94) and solid discriminative ability (ROC AUC = 0.83), which is ideal for a churn detection use case. \n",
    "\n",
    "Precision was expectedly lower due to the aggressive threshold of 0.1, but this tradeoff is acceptable when the business goal is to retain as many at-risk customers as possible.\n",
    "\n",
    "SMOTE and threshold adjustment were pivotal to achieving these results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
